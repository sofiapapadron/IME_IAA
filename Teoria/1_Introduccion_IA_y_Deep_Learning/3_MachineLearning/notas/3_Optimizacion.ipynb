{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color: rgb(50,120, 229);\"> Optimización con Descenso del Gradiente </font>\n",
    "\n",
    "En este cuaderno, utilizaremos un ejemplo simple para demostrar un algoritmo llamado Descenso del Gradiente. \n",
    "\n",
    "El descenso del gradiente es un algoritmo de optimización basado en el gradiente que se utiliza ampliamente en el aprendizaje automático y el aprendizaje profundo para minimizar una función de pérdida ajustando iterativamente los parámetros del modelo en la dirección del descenso más pronunciado basado en el gradiente negativo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Específicamente, veremos cómo ajustar una línea recta a través de un conjunto de puntos para determinar la pendiente de la línea. \n",
    "\n",
    "Para hacer esto, definiremos una **función de pérdida** que cuantifique el error entre los datos y el **modelo matemático** que elegimos para representar los datos, y usaremos esta función de pérdida para desarrollar una regla de **actualización** que convergerá iterativamente al valor óptimo. \n",
    "\n",
    "Concluiremos el cuaderno con una variación del algoritmo de Descenso del Gradiente llamada Descenso del Gradiente Estocástico en Mini-Batches, que es la base para el entrenamiento de redes neuronales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/c3_w1_gradient_descent_demo.gif\" width=\"700\"  />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color: rgb(50,120, 229);\">  ¿Qué es optimización? </font>\n",
    "\n",
    "La optimización es el proceso de encontrar el mejor resultado posible bajo ciertas circunstancias. \n",
    "\n",
    "En el aprendizaje automático, la optimización se refiere a la tarea de ajustar los parámetros de un modelo para minimizar una función de pérdida.\n",
    "\n",
    "## <font style=\"color: rgb(50,120, 229);\">  ¿Qué es el gradiente? </font>\n",
    "\n",
    "El gradiente es un vector que apunta en la dirección de un máximo local de una función y su magnitud indica la tasa de cambio de la función en esa dirección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15, 6)\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['axes.labelsize'] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color: rgb(50,120, 229);\"> 1. Crear un conjunto de datos </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_data():\n",
    "    # Seed manual para consistencia.\n",
    "    np.random.seed(42)\n",
    "\n",
    "    num_data = 30\n",
    "\n",
    "    # Crear datos que son aproximadamente lineales (pero no exactamente).\n",
    "    x = 10 * np.random.uniform(size=num_data)\n",
    "    y = x + np.random.normal(scale=0.3, size=num_data)\n",
    "\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Crear algunos datos.\n",
    "x, y = create_data()\n",
    "\n",
    "# Generar los datos para la línea inicial con una pendiente de 2.\n",
    "xmin = np.min(x)\n",
    "xmax = np.max(x)\n",
    "\n",
    "xplot = np.linspace(xmin, xmax, 2)\n",
    "m0 = 2\n",
    "yplot = m0 * xplot\n",
    "\n",
    "# Graficar los datos de muestra y la suposición inicial para una línea.\n",
    "plt.figure()\n",
    "plt.scatter(x, y, color='blue', s=20)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.plot(xplot, yplot, 'c--')\n",
    "plt.title('Datos de Muestra con Línea Inicial')\n",
    "plt.text(1, 7, 'Pendiente Inicial de la Línea: ' + str(m0), fontsize=14)\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(0, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color: rgb(50,120, 229);\"> 2. Definir el modelo </font>\n",
    "\n",
    "Nuestro modelo para los datos es una línea recta, y simplificaremos el problema para que la línea pase por el origen. \n",
    "\n",
    "La ecuación para tal línea es:\n",
    "\n",
    "$$y = mx$$\n",
    "\n",
    "El modelo tiene un único parámetro desconocido `m` (la pendiente de la línea) que deseamos calcular.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color: rgb(50,120, 229);\"> 3. Definir la función de pérdida </font>\n",
    "\n",
    "Ahora definamos una función de pérdida que cuantifique el error entre nuestro modelo y cualquier punto de datos en particular. Para cualquier valor dado de `xi` en nuestro conjunto de datos, tenemos el valor correspondiente para `yi` así como una estimación dada por `mxi`. Entonces, tendremos un error o un residuo dado por:\n",
    "\n",
    "$$\n",
    "\\text{error} = y_i - m x_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queremos encontrar un valor de `m` que minimice el error anterior. Los valores positivos o negativos del error son igualmente malos para nosotros. Entonces, si elevamos al cuadrado el error, podemos definir una métrica de pérdida que mide igualmente los errores en cualquier dirección (por encima o por debajo de la línea).\n",
    "\n",
    "La línea que mejor se ajusta al conjunto de datos en su conjunto minimizaría la pérdida total en todo el conjunto de datos, por lo que queremos sumar los errores para cada punto en el conjunto de datos. En otras palabras, queremos minimizar la siguiente ecuación:\n",
    "\n",
    "$$\n",
    "\\text{loss} = \\sum_{i=1}^{n} (y_i - m x_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto se conoce como la función de pérdida de suma de errores al cuadrado. Si calculamos la pérdida cuadrada promedio en todo el conjunto de datos, entonces llegamos a la función de pérdida del error cuadrático medio (MSE) que se muestra a continuación:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - m x_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra variación para la pérdida es el error medio absoluto (MAE) que se calcula de la siguiente manera:\n",
    "\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - m x_i|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TIP:**\n",
    "\n",
    "Una diferencia clave entre MSE y MAE es que la función de pérdida MSE es más sensible a los valores atípicos en el conjunto de datos. \n",
    "\n",
    "Si deseas minimizar el efecto de los valores atípicos en los datos, entonces MAE suele ser una mejor opción para una función de pérdida porque los errores no están al cuadrado como lo estarían con MSE. \n",
    "\n",
    "En el resto de este cuaderno, utilizaremos la función de pérdida MSE para demostrar el descenso del gradiente. Hay otros tipos de funciones de pérdida sobre los que aprenderemos más adelante en el curso.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color: rgb(50,120, 229);\"> 4. Optimización </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_linear_model(x, y, m_best, xlim=(0, 10), ylim=(0, 10)):\n",
    "    # Generar la línea basada en la pendiente óptima.\n",
    "    xmin = np.min(x)\n",
    "    xmax = np.max(x)\n",
    "    ymin = np.min(y)\n",
    "    ymax = np.max(y)\n",
    "\n",
    "    xplot = np.linspace(xmin, xmax, 2)\n",
    "    yplot = m_best * xplot\n",
    "\n",
    "    # Graficar los datos y el modelo.\n",
    "    plt.figure()\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)\n",
    "    plt.plot(xplot, yplot, 'c-')\n",
    "    plt.scatter(x, y, color='blue', s=20)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    xc = .05 * (xmax - xmin)\n",
    "    yc = .95 * (ymax - ymin)\n",
    "    plt.text(xc, yc, 'Pendiente: ' + str(round(m_best, 3)), fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font style=\"color: rgb(50,120, 229);\"> 4.1 Descenso del Gradiente </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora discutamos cómo funciona el descenso del gradiente. Para un valor dado de `m`, podemos calcular el gradiente de la función de pérdida y usar ese valor para informarnos cómo ajustar `m`. Si el gradiente es positivo, entonces necesitaremos disminuir el valor de `m` para acercarnos al mínimo, y si el gradiente es negativo, necesitaremos aumentar el valor de `m`. Esta idea simple se llama Descenso del Gradiente.\n",
    "\n",
    "Suponiendo que la función de pérdida es convexa y diferenciable, podemos calcular el gradiente de la función de pérdida con respecto a `m` en cualquier punto para lograr esto.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{MSE}}{\\partial m} = -\\frac{2}{n} \\sum_{i=1}^{n} x_i(y_i - m x_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observa que estamos calculando el gradiente para cada punto en el conjunto de datos, por eso esta técnica también se conoce como **Descenso del Gradiente en Lote**, ya que estamos procesando un \"lote\" de datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos usar el gradiente para desarrollar una regla de actualización para `m`. Para seguir la pendiente de la curva hacia el mínimo, necesitamos mover `m` en la dirección del gradiente negativo. Sin embargo, necesitamos controlar la velocidad a la que nos movemos a lo largo de la curva para no sobrepasar el mínimo.\n",
    "\n",
    "Por lo tanto, usamos un parámetro, `λ`, llamado tasa de aprendizaje. Este es un parámetro que requiere ajuste dependiendo del problema en cuestión.\n",
    "\n",
    "$$\n",
    "m_k = m_{k - 1} - \\lambda \\frac{\\partial \\text{MSE}}{\\partial m}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de parámetros.\n",
    "num_iter0 = 50\n",
    "lr0 = 0.005\n",
    "\n",
    "# Valor inicial de la pendiente.\n",
    "m0 = 2\n",
    "\n",
    "max_loss = 30. # Valor arbitrario para la pérdida máxima (solo para la visualización)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_iter = num_iter0\n",
    "lr = lr0\n",
    "m = m0\n",
    "\n",
    "# Inicializar el array para la pérdida en cada iteración.\n",
    "loss_gd = np.zeros(num_iter)\n",
    "\n",
    "# Calcular la pérdida.\n",
    "for i in range(num_iter):\n",
    "    # Calcular el gradiente usando todo el conjunto de datos.\n",
    "    g = -2 * np.sum(x * (y - m * x)) / len(x)\n",
    "\n",
    "    # Actualizar el parámetro, m.\n",
    "    m = m - lr * g\n",
    "\n",
    "    # Calcular la pérdida para el valor actualizado de m.\n",
    "    e = y - m * x\n",
    "    loss_gd[i] = np.sum(e * e) / len(x)\n",
    "\n",
    "m_best = m\n",
    "\n",
    "print('Minimum loss:   ', loss_gd[-1])\n",
    "print('Best parameter: ', m_best)\n",
    "\n",
    "# Graficar la pérdida vs iteraciones.\n",
    "plt.figure()\n",
    "plt.plot(loss_gd, 'c-')\n",
    "plt.xlim(0, num_iter)\n",
    "plt.ylim(0, np.max(loss_gd))\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.title('Gradient Descent')\n",
    "plt.show()\n",
    "\n",
    "plot_linear_model(x, y, m_best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font style=\"color: rgb(50,120, 229);\"> 4.2 Descenso del Gradiente Estocástico </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, solo tenemos un puñado de puntos de datos. En el mundo real, podemos tener **millones de ejemplos**. Calcular el gradiente basado en todos los puntos de datos puede ser computacionalmente costoso. Afortunadamente, usar todos los puntos de datos para calcular el gradiente es innecesario.\n",
    "\n",
    "Podemos usar **un solo punto** de datos elegido al azar para calcular el gradiente en cada iteración. Aunque el gradiente en cada paso no es tan preciso, la idea sigue funcionando. La convergencia podría ser más lenta utilizando esta técnica porque el gradiente no es tan preciso. En la próxima sección, ampliaremos esta idea para usar un pequeño porcentaje de los datos para aproximar mejor el gradiente y limitar el número de cálculos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_iter = num_iter0\n",
    "lr = lr0\n",
    "m = m0\n",
    "\n",
    "# Inicializar el array para la pérdida en cada iteración.\n",
    "loss_sgd = np.zeros(num_iter)\n",
    "\n",
    "for i in range(num_iter):\n",
    "    # Seleccionar aleatoriamente un punto de datos de entrenamiento.\n",
    "    k = np.random.randint(0, len(y))\n",
    "\n",
    "    # Calcular el gradiente usando un solo punto de datos.\n",
    "    g = -2 * x[k] * (y[k] - m * x[k])\n",
    "\n",
    "    # Actualizar el parámetro, m.\n",
    "    m = m - lr * g\n",
    "\n",
    "    # Calcular la pérdida para el valor actualizado de m.\n",
    "    e = y - m * x\n",
    "    loss_sgd[i] = np.sum(e * e)\n",
    "\n",
    "m_best = m\n",
    "\n",
    "print('Minimum loss:   ', loss_sgd[-1])\n",
    "print('Best parameter: ', m_best)\n",
    "\n",
    "# Graficar la pérdida vs iteraciones.\n",
    "plt.figure()\n",
    "plt.plot(loss_sgd, 'c-')\n",
    "plt.xlim(0, num_iter)\n",
    "plt.ylim(0, np.max(loss_sgd))\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.title('Stochastic Gradient Descent')\n",
    "plt.show()\n",
    "\n",
    "plot_linear_model(x, y, m_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font style=\"color: rgb(50,120, 229);\"> 4.3 Descenso del Gradiente Estocástico en Mini-Batches </font>\n",
    "\n",
    "En la sección anterior, vimos que es posible calcular el gradiente basado en un solo punto de datos aleatorio elegido en cada iteración. Siempre y cuando ejecutemos suficientes iteraciones, el Descenso del Gradiente Estocástico seguirá funcionando.\n",
    "\n",
    "Sin embargo, usar más de un punto de datos para el cálculo del gradiente tiene dos ventajas:\n",
    "\n",
    "1. Usar múltiples puntos de datos produce una estimación más precisa del gradiente.\n",
    "2. Las GPUs son altamente eficientes en el procesamiento de cálculos de gradiente.\n",
    "\n",
    "Por lo tanto, obtenemos mejores resultados y una convergencia más rápida si usamos un pequeño lote de puntos de datos, llamado **mini-lote**, para calcular los gradientes. Un enfoque de \"mini-lote\" encuentra un buen equilibrio entre usar todos los puntos de datos vs. solo un punto de datos.\n",
    "\n",
    "Implementemos esto en código y veamos por nosotros mismos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_iter = num_iter0\n",
    "lr = lr0\n",
    "m = m0\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "# Inicializar el array para la pérdida en cada iteración.\n",
    "loss_sgd_mb = np.zeros(num_iter)\n",
    "\n",
    "for i in range(num_iter):\n",
    "    # Seleccionar aleatoriamente un batch de puntos de datos.\n",
    "    k = np.random.randint(0, len(y), size=batch_size)\n",
    "\n",
    "    # Calcular el gradiente usando el mini-batch.\n",
    "    g = -2 * np.sum(x[k] * (y[k] - m * x[k])) / batch_size\n",
    "\n",
    "    # Actualizar el parámetro, m.\n",
    "    m = m - lr * g\n",
    "\n",
    "    # Calcular la pérdida para el valor actualizado de m.\n",
    "    e = y - m * x\n",
    "    loss_sgd_mb[i] = np.sum(e * e) / batch_size\n",
    "\n",
    "m_best = m\n",
    "\n",
    "print('Minimum loss:   ', loss_sgd_mb[-1])\n",
    "print('Best parameter: ', m_best)\n",
    "\n",
    "# Graficar la pérdida vs iteraciones.\n",
    "plt.figure()\n",
    "plt.plot(loss_sgd_mb, 'c-')\n",
    "plt.xlim(0, num_iter)\n",
    "plt.ylim(0, np.max(loss_sgd_mb))\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.title('Stochastic Gradient Descent with Mini-Batch')\n",
    "plt.show()\n",
    "\n",
    "plot_linear_model(x, y, m_best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
