{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color: rgb(50, 120, 229)\"> ¿Cómo aprende una red neuronal? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este cuaderno vamos a aprender un algoritmo muy importante en el aprendizaje profundo llamado backpropagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a empezar con una vista general de una red neuronal y consideremos un problema de clasificación de imágenes como ejemplo.\n",
    "\n",
    "La entrada es una imagen de un animal y la salida es el nombre del animal o la etiqueta de clase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos pensar en la red neuronal como una caja negra con perillas. Estas perillas se llaman pesos. Cuando los pesos son correctos, obtenemos los resultados correctos con más frecuencia. La pregunta es, *¿cómo obtenemos los valores correctos para los pesos?*\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"./images/blackbox.png\" width=\"800px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font style=\"color: rgb(50, 120, 229)\"> **¿Cómo sabemos el valor de los pesos?** </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bueno, **entrenamos la red neuronal**. \n",
    "\n",
    "Mostramos miles de ejemplos de diferentes animales y le decimos a la red neuronal el nombre del animal en cada imagen.\n",
    "\n",
    "<center>\n",
    "<img src=\"./images/samples.png\" width=\"800px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después definimos una **función de pérdida**. \n",
    "\n",
    "La función de pérdida no es más que una función que mide el error de clasificación. Es alta cuando la red comete muchos errores y es baja cuando la red no comete muchos errores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo general, la función de pérdida es convexa, en otras palabras, tiene forma de tazón con respecto a los pesos.\n",
    "\n",
    "<center>\n",
    "<img src=\"./images/bowl.png\" width=\"500px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos empezar con un **conjunto aleatorio de pesos**. \n",
    "\n",
    "Si calculamos la pendiente de este tazón con respecto a los pesos, podemos encontrar la dirección para minimizar la pérdida, la pendiente no es otra cosa que el **gradiente**.\n",
    "\n",
    "<center>\n",
    "<img src=\"./images/gradient.png\" width=\"800px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos controlar cuánto nos movemos a lo largo de la dirección por un parámetro Lambda, que se llama la tasa de aprendizaje.\n",
    "\n",
    "<center>\n",
    "<img src=\"./images/lr.png\" width=\"800px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando nos movemos en este espacio, obtenemos un nuevo conjunto de pesos. Podemos calcular el gradiente de la pérdida nuevamente en las nuevas ubicaciones de peso y encontrar la dirección en la que movernos para minimizar la pérdida nuevamente.\n",
    "\n",
    "<center>\n",
    "<img src=\"./images/updated.png\" width=\"800px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos seguir actualizando los pesos hasta que lleguemos al fondo del tazón donde el gradiente es cero y, por lo tanto, los pesos dejan de actualizarse.\n",
    "\n",
    "<center>\n",
    "<img src=\"./images/stop.png\" width=\"800px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto, decimos que el algoritmo de descenso de gradiente ha convergido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font style=\"color: rgb(50, 120, 229)\"> **¿Cómo calculamos el gradiente?** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí es donde entra en juego  el algoritmo de **backpropagation**.\n",
    "\n",
    "Nos ayuda a calcular eficientemente los gradientes de la función de pérdida con respecto a los pesos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation no es más que la regla de la cadena del cálculo aplicada repetidamente, así que vamos a repasar rápidamente la regla de la cadena."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font style=\"color: rgb(50, 120, 229)\"> **Regla de la cadena** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí tienes la versión más simple de la regla de la cadena: para encontrar la derivada de Z respecto a X, puedes multiplicar la derivada de Z respecto a Y con la derivada de Y respecto a X.\n",
    "\n",
    "$$ z = f(y) $$\n",
    "$$ y = g(x) $$\n",
    "\n",
    "$$\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y} \\cdot \\frac{\\partial y}{\\partial x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color: rgb(50, 120, 229)\"> Backpropagation en una neurona </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a aprender la retropropagación utilizando un solo neurona. La neurona tiene dos entradas X1 y X2 con pesos W1 y W2. También tiene un término de sesgo B, por ahora ignoremos la función de activación.\n",
    "\n",
    "La salida de la neurona es:\n",
    "\n",
    "$$ Y = W1 \\cdot X1 + W2 \\cdot X2 + B $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este paso de cálculo se llama **forward pass** o **forward propagation** (propagación hacia adelante).\n",
    "\n",
    "<center>\n",
    "<img src=\"./images/forward.png\" width=\"600px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font style=\"color: rgb(50, 120, 229)\"> **Visualizando la neurona como un grafo computacional** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También puedes ver la neurona como un grafo computacional. \n",
    "\n",
    "En este grafo, las entradas y los pesos son nodos que son operados por operaciones mostradas con círculos verdes. \n",
    "\n",
    "Puedes realizar la propagación hacia adelante visitando todos los nodos y realizando todas las operaciones necesarias.\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"./images/graph.png\" width=\"600px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por otro lado, en backpropagation estamos interesados en propagar los gradientes hacia atrás utilizando la regla de la cadena. \n",
    "\n",
    "Para backpropagation, comenzamos al final del grafo computacional.\n",
    "\n",
    "La derivada de $y$ con respecto a $y$ es 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora estamos interesados en la derivada de la salida con respecto a los pesos. Comencemos mirando primero el nodo de procesamiento final.\n",
    "\n",
    "Es un nodo de adición para el cual podemos escribir la ecuación \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"./images/back1.png\" width=\"600px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eso nos lleva al siguiente nodo de adición."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"./images/back2.png\" width=\"800px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuamos con el bloque de multiplicación.\n",
    "\n",
    "<center>\n",
    "<img src=\"./images/back3.png\" width=\"800px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color: rgb(50, 120, 229)\"> Pasos para entrenar una red neuronal </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Empezamos con un conjunto aleatorio de pesos.\n",
    "2. Seleccionamos un conjunto de datos para entrenar.\n",
    "3. Usamos propagación hacia adelante para obtener la salida de la red.\n",
    "4. Calculamos los gradientes utilizando backpropagation.\n",
    "5. Utilizamos descenso de gradiente para actualizar los pesos.\n",
    "6. Repetimos los pasos 3-5 hasta que la red haya convergido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya estas listo para entrenar tu primera red neuronal. Dirigete al siguiente cuaderno [TensorFlow Playground](../ejercicios/1_Playground.ipynb) para entrenar una red neuronal en un entorno interactivo."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
