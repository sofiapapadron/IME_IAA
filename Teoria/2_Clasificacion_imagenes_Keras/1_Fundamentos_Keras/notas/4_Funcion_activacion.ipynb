{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:rgb(50,120,229)\"> Funciones de activación </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "block_plot = False\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_activation(x, y, y_label, title):\n",
    "    plt.figure\n",
    "    plt.plot(x, y, color='b')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    plt.show(block=block_plot)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(50,120,229)\"> Función Sigmoid </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función Sigmoide se define como:\n",
    "\n",
    "$$\n",
    "y' = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Es más adecuada para la **clasificación binaria**. Por lo tanto, podemos inferir algunas cosas de lo siguiente:\n",
    "\n",
    "- Si $sigmoid(z) > 0.5$, entonces la entrada pertenece a la clase positiva o clase `1`.\n",
    "- Si $sigmoid(z) < 0.5$, entonces la entrada pertenece a la clase negativa o clase `0`.\n",
    "\n",
    "La salida `sigmoid` $y'$ se puede pensar como la probabilidad de que el punto de datos pertenezca a la clase `1`. Entonces, la probabilidad de que pertenezca a la clase `0` será $1-y'$.\n",
    "\n",
    "Se usa principalmente en la capa final de una red neuronal.\n",
    "\n",
    "Una de las mayores desventajas de la activación Sigmoide es el **problema de desvanecimiento del gradiente**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sigmoid activation function.\n",
    "x = tf.linspace(-10, 10, 1000)\n",
    "y = tf.nn.sigmoid(x)\n",
    "print(y[:10])\n",
    "\n",
    "plot_activation(x, y, 'sigmoid(x)', 'Sigmoid Activation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font style=\"color:rgb(8,133,37)\"> **Sintaxis en Keras** </font>\n",
    "\n",
    "```python\n",
    "tf.keras.activations.sigmoid(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(50,120,229)\"> Función ReLU </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cualquier entrada dada, la función de activación ReLU (Rectified Linear Unit) devuelve 0 o el mismo valor que la entrada. La fórmula es la siguiente:\n",
    "\n",
    "$$\n",
    "ReLU(x) = max(0, x)\n",
    "$$\n",
    "\n",
    "**Entonces, ¿cuándo devuelve 0?** *Siempre que el valor de entrada sea menor que 0, devuelve 0, de lo contrario siempre devuelve el mismo valor que la entrada*.\n",
    "\n",
    "Desglosando la explicación anterior en una simple declaración $if$, se verá algo así:\n",
    "\n",
    "$$\n",
    "   ReLU(x) = \n",
    "\\begin{cases}\n",
    "    0,& \\text{si } x < 0\\\\\n",
    "    x,              & \\text{en otro caso}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Es la función de activación más utilizada en la actualidad**.\n",
    "\n",
    "Ahora, implementemos ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot relu activation function.\n",
    "x = tf.linspace(-10, 10, 1000)\n",
    "y = tf.nn.relu(x)\n",
    "print(y[:10])\n",
    "\n",
    "plot_activation(x, y, 'relu(x)', 'ReLU Activation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font style=\"color:rgb(8,133,37)\"> **Sintaxis en Keras** </font>\n",
    "\n",
    "```python\n",
    "tf.keras.activations.relu(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(50,120,229)\"> Función Tanh </font>\n",
    "\n",
    "La función de activación tangente hiperbólica (tanh) es algo similar a la función de activación sigmoide, al menos en cuanto a la gráfica.\n",
    "\n",
    "Pero en lugar del rango de salida siendo entre 0 y 1, varía de -1 a 1.\n",
    "\n",
    "Y lo siguiente da la fórmula para la activación tanh:\n",
    "\n",
    "$$\n",
    "tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$\n",
    "\n",
    "El siguiente bloque de código muestra la implementación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problemas**:\n",
    "\n",
    "- Presenta desvanecimiento del gradiente.\n",
    "- Es computacionalmente costosa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tanh activation function.\n",
    "x = tf.linspace(-10, 10, 1000)\n",
    "y = tf.nn.tanh(x)\n",
    "print(y[:10])\n",
    "\n",
    "plot_activation(x, y, 'tanh(x)', 'Tanh Activation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font style=\"color:rgb(8,133,37)\"> **Sintaxis en Keras** </font>\n",
    "\n",
    "```python\n",
    "tf.keras.activations.tanh(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(50,120,229)\"> Función ELU </font>\n",
    "\n",
    "ELU significa Unidad Lineal Exponencial. En comparación con ReLU, esto también tiene una constante alfa ($\\alpha$) que aplica un poco de no linealidad cuando los valores son negativos.\n",
    "\n",
    "Podemos calcular ELU como:\n",
    "\n",
    "$$\n",
    "   ELU(x) = \n",
    "\\begin{cases}\n",
    "    \\alpha(e^x - 1),& \\text{si } x < 0\\\\\n",
    "    x,              & \\text{en otro caso}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Tiene algunas ventajas sobre ReLU, como:\n",
    "* ELU es menos propenso a tener un problema de gradiente explosivo como a veces puede ser el caso con ReLU.\n",
    "* Además, ELU no sufre del problema de *relu muerto* como lo hace ReLU.\n",
    "\n",
    "Aún así, una gran desventaja de ELU es que es más lento de calcular porque también aplica no linealidad a las entradas negativas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot elu activation function.\n",
    "x = tf.linspace(-10, 10, 1000)\n",
    "y = tf.nn.elu(x)\n",
    "print(y[:10])\n",
    "\n",
    "plot_activation(x, y, 'elu(x)', 'ELU Activation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font style=\"color:rgb(8,133,37)\"> **Sintaxis en Keras** </font>\n",
    "\n",
    "```python\n",
    "tf.keras.activations.elu(x, alpha=1.0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(50,120,229)\"> Función Softmax </font>\n",
    "\n",
    "Aunque podemos llamar a Softmax una función de activación, *realmente no es una función de activación*.\n",
    "\n",
    "La función Softmax devuelve la distribución de probabilidad de las entradas dadas. Típicamente, las entradas pueden ser valores de -∞ a +∞. Pero después de que se haya aplicado la activación Softmax, los valores de salida están entre 0 y 1.\n",
    "\n",
    "Y se usa principalmente en la última capa de la red neuronal para **problemas de clasificación multiclase**. \n",
    "\n",
    "La función Softmax toma los logitos (salidas directas de la red neuronal) y los convierte en una distribución de probabilidad para operaciones posteriores.\n",
    "\n",
    "Echemos un vistazo a la fórmula:\n",
    "\n",
    "$$\n",
    "Softmax(x_i) = \\frac{exp(x_i)}{\\sum_{i=0}^{n}exp(x_i)}\n",
    "$$\n",
    "\n",
    "Ahora, implementemos lo mismo usando Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font style=\"color:rgb(8,133,37)\"> **Sintaxis en Keras** </font>\n",
    "\n",
    "```python\n",
    "tf.keras.activations.softmax(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot softmax activation function.\n",
    "x = tf.constant([-3, 5, 1], dtype=tf.float32)\n",
    "y = tf.nn.softmax(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"./images/softmax.webp\" width=600>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
