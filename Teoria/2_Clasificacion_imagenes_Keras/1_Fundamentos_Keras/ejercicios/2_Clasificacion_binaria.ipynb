{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:rgb(50,120,229)\"> Clasificación Binaria en Keras </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este cuaderno, introduciremos el tema de la **clasificación binaria**. \n",
    "\n",
    "En el aprendizaje automático, la clasificación se refiere al problema de modelado predictivo de identificar a cuál de un conjunto de categorías pertenece una observación. Las categorías se denominan clases. \n",
    "\n",
    "En la figura siguiente, mostramos un conjunto de datos hipotético que representa dos clases. Hemos codificado por colores las clases como rojo y azul, pero esto es solo para fines de visualización. Las clases en sí mismas están caracterizadas por dos características ($x1$ y $x2$). \n",
    "\n",
    "Nuestra tarea es definir un modelo que prediga la clase correcta en función del valor de dos características de entrada.\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"./images/binary_casslification.jpg\" width=\"800px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt  # one of the best graphics library for python\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 6)\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "block_plot = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (2, )\n",
    "DATA_PATH = './data/binary_classification_data.npy'\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(50,120,229)\">1. Carga de datos </font>\n",
    "\n",
    "En este ejemplo cargaremos un conjunto de datos de clasificación binaria. Este conjunto de datos tiene dos características (x1 y x2) y una etiqueta de clase (0 o 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the data\n",
    "plt.scatter(data[:,0], data[:,1], c=data[:,2], cmap='coolwarm', s=10)\n",
    "plt.title('Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(50,120,229)\">2. Preparación de datos </font>\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos que generamos tienen tres columnas: x1, x2 y y. Las dos primeras columnas son las características y la última columna es la etiqueta de clase.\n",
    "\n",
    "Dividiremos los datos en:\n",
    "\n",
    "- X_train: características de entrenamiento\n",
    "- y_train: etiquetas de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data[:,0:2]\n",
    "y_train = data[:,2]\n",
    "\n",
    "#Mezclar los datos\n",
    "idx = np.arange(X_train.shape[0])\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "X_train = X_train[idx]\n",
    "y_train = y_train[idx]\n",
    "\n",
    "\n",
    "print(f\"Datos de entrenamiento: {X_train.shape}\")\n",
    "print(f\"Etiquetas de entrenamiento: {y_train.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(50,120,229)\">3. Normalización de datos </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de comenzar con nuestro entrenamiento real, necesitamos normalizar nuestros datos utilizando su media y desviación estándar.\n",
    "\n",
    "**La normalización ayuda a que el modelo converja más rápido.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La normalización que aplicaremos es la **estandarización**:\n",
    "\n",
    "$$x_{\\text{estandarizado}} = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "donde $\\mu$ es la media y $\\sigma$ es la desviación estándar.\n",
    "\n",
    "<font style=\"color:rgb(8,133,37)\">**Sintaxis:**</font>\n",
    "\n",
    "```python\n",
    "from keras.layers import Normalization\n",
    "\n",
    "normalizer = Normalization(axis=-1)\n",
    "\n",
    "# adaptamos el normalizador a los datos\n",
    "normalizer.adapt(data)\n",
    "```\n",
    "\n",
    "**Parámetros:**\n",
    "\n",
    "- `data:` Array de NumPy\n",
    "- `axis:` Eje a lo largo del cual se calcula la media y la desviación estándar (1 para columnas y -1 para filas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Obtener la media y la desviación estándar de los datos de entrenamiento\n",
    "\n",
    "#TODO: Crear una capa de normalización y adaptarla a los datos de entrenamiento\n",
    "\n",
    "\n",
    "print(f\"Media: {normalizer.mean.numpy()}\")\n",
    "print(f\"Desviación estándar: {normalizer.variance.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(50,120,229)\">4. Arquitectura modelo </font>\n",
    "\n",
    "La arquitectura del modelo para clasificación binaria que se muestra a continuación es similar a la regresión, pero resaltemos las diferencias. \n",
    "\n",
    "- En primer lugar, observe que usamos una función de activación sigmoide ya que la salida predicha es una etiqueta en lugar de un número de valor real. \n",
    "  \n",
    "- La etiqueta indica qué clase se predice. En este ejemplo, suponemos clasificación binaria, por lo que tenemos dos etiquetas (digamos, 0 y 1). \n",
    "  \n",
    "- Tenga en cuenta que la clasificación binaria puede usar cualquier cantidad de características de entrada. En este cuaderno, usaremos dos características de entrada, pero esto no tiene nada que ver con el hecho de que estemos realizando clasificación binaria.\n",
    "\n",
    "<center>\n",
    "<img src=\"./images/binary_architecture.webp\" width=\"800px\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Crea un modelo secuencial con una capa de entrada de 2 neuronas\n",
    "\n",
    "#TODO: Añade la capa de normalización definida anteriormente\n",
    "\n",
    "#TODO: Añade una capa densa con 1 neurona y función de activación sigmoide (sigmoid)\n",
    "\n",
    "#TODO: Muestra un resumen del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuerda que el flujo de entrenamiento es el siguiente:\n",
    "\n",
    "<center>\n",
    "<img src=\"./images/keras_training_workflow.webp\" width=\"600px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(50,120,229)\">5. Compilar el modelo </font>\n",
    "\n",
    "Recuerda que compilar el modelo se refiere a especificar una función de pérdida y un optimizador.\n",
    "\n",
    "- Usaremos la función de pérdida de entropía cruzada binaria.\n",
    "- Usaremos el optimizador SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Compila el modelo con optimizador SGD y función de pérdida binary_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(50,120,229)\">7. Entrenamiento del modelo </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuerda que el entrenamiento del modelo implica llamar al método `fit` en el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Entrena el modelo con los datos de entrenamiento y 100 épocas, guarda el historial en la variable history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(range(len(loss)), loss)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlim([0, 100])\n",
    "plt.ylim([0, 1])\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show(block=block_plot)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(50,120,229)\">9. Visualización de la frontera de decisión </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a generar nuevos datos y ver como el modelo clasifica estos datos. **Esto solo es con fines de visualización, no es parte del proceso de entrenamiento habitual.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.layers[1].get_weights()[0]\n",
    "w1 = weights[0][0]\n",
    "w2 = weights[1][0]\n",
    "b  = model.layers[1].get_weights()[1] \n",
    "    \n",
    "print('Weights associated with normalized data')\n",
    "print('b: ',b)\n",
    "print('w1: ',w1)\n",
    "print('w2: ',w2)\n",
    "print('\\n')\n",
    "\n",
    "# Retrieve normalization statistics.\n",
    "norm_mean = normalizer.mean.numpy()[0]\n",
    "norm_var = normalizer.variance.numpy()[0]\n",
    "\n",
    "std_1 = math.sqrt(norm_var[0])\n",
    "std_2 = math.sqrt(norm_var[1])\n",
    "\n",
    "mean_1 = norm_mean[0]\n",
    "mean_2 = norm_mean[1]\n",
    "\n",
    "# Unnormalize the weights for use in diagnostics.\n",
    "w1 = w1/std_1\n",
    "w2 = w2/std_2\n",
    "b = b - w1*mean_1 - w2*mean_2\n",
    "    \n",
    "print('Weights associated with unnormalized data')\n",
    "print('b: ',b)\n",
    "print('w1: ',w1)\n",
    "print('w2: ',w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0_points = data[data[:, 2] == 0][:, 0:2]\n",
    "class_1_points = data[data[:, 2] == 1][:, 0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.scatter(class_0_points[:, 0], class_0_points[:, 1], color=\"b\", alpha=0.5, label=\"Class:0\")\n",
    "plt.scatter(class_1_points[:, 0], class_1_points[:, 1], color=\"r\", alpha=0.5, label=\"Class:1\")\n",
    "\n",
    "x1 = tf.linspace(0.0, 10.0, 1000)\n",
    "x2 = -(w1/w2)*x1 - b/w2\n",
    "\n",
    "plt.plot(x1, x2, c=\"black\", alpha=.5)\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.xlim([0, 10])\n",
    "plt.ylim([16, 28])\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show(block=block_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(50,120,229)\"> Preguntas de comprensión </font>\n",
    "\n",
    "Responder las siguientes preguntas:\n",
    "\n",
    "1. ¿Qué es la clasificación binaria?\n",
    "2. ¿Por queé fue importante mezclar los datos antes de entrenar el modelo?\n",
    "3. ¿Que función de activación se utiliza en la capa de salida de un modelo de clasificación binaria?\n",
    "4. ¿Qué función de pérdida se utiliza en un modelo de clasificación binaria?\n",
    "5. ¿Qué ventaja tiene la normalización de datos en el entrenamiento de un modelo?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
