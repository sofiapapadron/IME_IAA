{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:rgb(50, 120, 229);\">  Regularización en Keras </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este cuaderno, adoptaremos un enfoque experimental para aprender cómo usar conceptos para mejorar nuestro modelo. \n",
    "\n",
    "Ilustraremos las diferentes técnicas usando el conjunto de datos Fashion MNIST.\n",
    "<center>\n",
    "<img src=\"./images/MNIST_fashion.png\" width=\"500\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout, Conv2D, MaxPooling2D, Flatten, Input, Normalization, Resizing\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric(history, title):\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.suptitle(title)\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(loss, label='loss')\n",
    "    plt.plot(val_loss, label='val_loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(acc, label='accuracy')\n",
    "    plt.plot(val_acc, label='val_accuracy')\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(50, 120, 229);\">  Experimento 1: Usar un modelo simple </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comienza con una CNN pequeña (SmallModel [S]). \n",
    "\n",
    "Ten en cuenta que si la precisión de entrenamiento no es muy alta, el modelo no es lo suficientemente complejo como para sobreajustar (obtener casi un 100% de precisión en los datos de entrenamiento o alrededor de cero pérdida en los datos de entrenamiento). \n",
    "\n",
    "Siguiendo la definición de sesgo y varianza, este modelo tiene sesgo alto y varianza alta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model = Sequential([\n",
    "   Input(shape=(28, 28, 1)),\n",
    "   Normalization(),\n",
    "   Conv2D(filters=4, kernel_size=5, activation='relu'),\n",
    "   MaxPooling2D(2),\n",
    "   Conv2D(filters=8, kernel_size=3, activation='relu'),\n",
    "   MaxPooling2D(2),\n",
    "   Flatten(),\n",
    "   Dense(100, activation='relu'),\n",
    "   Dense(50, activation='relu'),\n",
    "   Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "simple_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_history = simple_model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(simple_history, 'Simple model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(50, 120, 229);\">  Experimento 2: Aumentar la complejidad del modelo </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, entrenaremos un modelo sin regularización (sin Dropout, Batch Norm, Aumento de datos, Programador de velocidad de aprendizaje o penalización L2). \n",
    "\n",
    "Descubrimos que ese modelo sobreajusta (100% de precisión en el entrenamiento, pero solo tiene 92-93% para la validación). Este modelo tiene sesgo bajo y varianza alta. Por lo tanto, necesitamos usar técnicas de regularización para reducir esta alta varianza.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_model = Sequential([\n",
    "    Input(shape=(28, 28, 1)),\n",
    "    Normalization(),\n",
    "    Conv2D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
    "    MaxPooling2D(2),\n",
    "    Conv2D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "    MaxPooling2D(2),\n",
    "    Conv2D(filters=256, kernel_size=3, activation='relu', padding='same'),\n",
    "    MaxPooling2D(2),\n",
    "    Conv2D(filters=512, kernel_size=3, activation='relu', padding='same'),\n",
    "    MaxPooling2D(2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "mid_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "mid_history = mid_model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(mid_history, 'Mid model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(50, 120, 229);\">  Experimento 3: Agregar Batch Normalization </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este experimento, agregaremos capas `BatchNormalization` a la parte del extractor de características del modelo. \n",
    "\n",
    "Notarás que la precisión de validación mejora, así como la pérdida disminuye. **Incluso si la precisión no mejora y solo la pérdida disminuye, aún significa que este es un modelo más robusto** (la confianza en la clasificación incorrecta no será muy alta).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_model_batch_norm = Sequential([\n",
    "    Input(shape=(28, 28, 1)),\n",
    "    Normalization(),\n",
    "    Conv2D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2),\n",
    "    Conv2D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2),\n",
    "    Conv2D(filters=256, kernel_size=3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2),\n",
    "    Conv2D(filters=512, kernel_size=3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "mid_model_batch_norm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_model_batch_norm.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "mid_history_batch_norm = mid_model_batch_norm.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(mid_history_batch_norm, 'Mid model with batch normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(50, 120, 229);\">  Experimento 4: Agregar Dropout </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En lugar de capas de normalización por lotes, utilizaremos Dropout2d para capas convolucionales y Dropout para capas lineales. \n",
    "\n",
    "Con las capas de dropout, la precisión de validación permanece casi igual, pero la pérdida mejora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_model_dropout = Sequential([\n",
    "    Input(shape=(28, 28, 1)),\n",
    "    Normalization(),\n",
    "    Conv2D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
    "    MaxPooling2D(2),\n",
    "    Dropout(0.5),\n",
    "    Conv2D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "    MaxPooling2D(2),\n",
    "    Dropout(0.5),\n",
    "    Conv2D(filters=256, kernel_size=3, activation='relu', padding='same'),\n",
    "    MaxPooling2D(2),\n",
    "    Dropout(0.5),\n",
    "    Conv2D(filters=512, kernel_size=3, activation='relu', padding='same'),\n",
    "    MaxPooling2D(2),\n",
    "    Dropout(0.5),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "mid_model_dropout.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_model_dropout.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "mid_history_dropout = mid_model_dropout.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(mid_history_dropout, 'Mid model with dropout')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(50, 120, 229);\">  Experimento 5: Batch Normalization y Dropout </font>\n",
    "\n",
    "Ahora, considera Batch Norm con Dropout. En términos de precisión de validación y pérdida, estos casos son ligeramente mejores que los anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_model_dropout_batch_norm = Sequential([\n",
    "    Input(shape=(28, 28, 1)),\n",
    "    Normalization(),\n",
    "    Conv2D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2),\n",
    "    Dropout(0.5),\n",
    "    Conv2D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2),\n",
    "    Dropout(0.5),\n",
    "    Conv2D(filters=256, kernel_size=3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2),\n",
    "    Dropout(0.5),\n",
    "    Conv2D(filters=512, kernel_size=3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2),\n",
    "    Dropout(0.5),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "mid_model_dropout_batch_norm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_model_dropout_batch_norm.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "mid_history_dropout_batch_norm = mid_model_dropout_batch_norm.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(mid_history_dropout_batch_norm, 'Mid model with dropout and batch normalization')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
