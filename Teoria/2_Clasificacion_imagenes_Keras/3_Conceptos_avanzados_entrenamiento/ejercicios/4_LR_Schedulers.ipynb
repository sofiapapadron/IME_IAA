{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:rgb(50,120,229)\"> Learning Rate Schedulers </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La tasa de aprendizaje ($\\lambda$) es un hiperparámetro que determina qué tan grande debe ser el paso que debemos tomar en cada iteración al actualizar los parámetros del modelo durante el entrenamiento.\n",
    "\n",
    "$$W_{i+1} = W_{i} - \\lambda * \\frac{\\partial L}{\\partial W}$$\n",
    "\n",
    "Al entrenar un modelo desde cero, generalmente comenzamos con una tasa de aprendizaje más alta, pero a medida que avanza el aprendizaje, reducir la tasa de aprendizaje puede ayudar a mejorar la velocidad de convergencia y a veces incluso resulta en una mayor precisión (pérdida más baja). \n",
    "\n",
    "En TensorFlow, podemos agregar programadores de tasa de aprendizaje usando el módulo [`tf.keras.optimizers.schedules`](https://keras.io/api/optimizers/learning_rate_schedules/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este cuaderno, aprenderemos sobre tres programadores de tasa de aprendizaje diferentes en TensorFlow utilizando los datos de [Rock, Paper, Scissors](https://www.tensorflow.org/datasets/catalog/rock_paper_scissors) de TensorFlow datasets [tfds](https://www.tensorflow.org/datasets/api_docs/python/tfds). Específicamente, echaremos un vistazo a los siguientes programadores de tasa de aprendizaje:\n",
    "\n",
    "- Decaimiento Constante Escalonado\n",
    "- Decaimiento Inverso en el Tiempo\n",
    "- Decaimiento Exponencial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(50,120,229)\"> 1. Descargar y Preprocesar los Datos </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = {\n",
    "    'Rock'     : 0, \n",
    "    'Paper'    : 1, \n",
    "    'Scissors' : 2, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "def get_dataset():\n",
    "    train_ds = tfds.load('rock_paper_scissors', split=tfds.Split.TRAIN, batch_size=-1)\n",
    "    test_ds = tfds.load('rock_paper_scissors', split=tfds.Split.TEST, batch_size=-1)\n",
    "\n",
    "    train_ds =tfds.as_numpy(train_ds)\n",
    "    test_ds =tfds.as_numpy(test_ds)\n",
    "\n",
    "    X_train, y_train = train_ds['image'], train_ds['label']\n",
    "    X_test, y_test = test_ds['image'], test_ds['label']\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X_train.shape: {X_train.shape}\")\n",
    "print(f\"y_train.shape: {y_train.shape}\")\n",
    "\n",
    "print(f\"X_test.shape: {X_test.shape}\")\n",
    "print(f\"y_test.shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos one-hot encoding a las etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "print(f\"y_train.shape: {y_train.shape}\")\n",
    "print(f\"y_test.shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(18, 8))\n",
    "    \n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i+1)\n",
    "    plt.axis('off')\n",
    "\n",
    "    image = X_train[i]\n",
    "    label = y_train[i].argmax()\n",
    "\n",
    "    class_name = list(label_names.keys())[list(label_names.values()).index(label)]\n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.title(class_name)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(50,120,229)\"> 2. Crear el Modelo </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPool2D, Flatten, Dense, Rescaling\n",
    "\n",
    "def LeNet5_model(num_classes, shape):\n",
    "\n",
    "    inputs = Input(shape=shape)\n",
    "\n",
    "    x = Rescaling(scale=1./255)(inputs)\n",
    "\n",
    "    x = Conv2D(6, kernel_size=(5, 5), activation='relu')(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    x = Conv2D(16, kernel_size=(5, 5), activation='relu')(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    x = Dense(120, activation='relu')(x)\n",
    "    x = Dense(84, activation='relu')(x)\n",
    "\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(50,120,229)\"> 3. Entrenar el Modelo </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "def train_model(X_train, y_train, X_test, y_test, model, optimizer, epochs=25):\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, callbacks=[early_stopping])\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_accuracy(history):\n",
    "    plt.figure(figsize=(18, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss evolution')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Accuracy evolution')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:rgb(50,120,229)\"> 3.1 Sin Programador de Tasa de Aprendizaje </font>\n",
    "\n",
    "En nuestro primer experimento, entrenaremos el modelo sin un programador de tasa de aprendizaje.\n",
    "\n",
    "Utilizaremos el optimizador Adam con una tasa de aprendizaje de 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "model = LeNet5_model(num_classes=3, shape=(300, 300, 3))\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "history = train_model(X_train, y_train, X_test, y_test, model, optimizer, epochs=25)\n",
    "\n",
    "plot_loss_accuracy(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:rgb(50,120,229)\"> 3.2 Piecewise Constant Decay </font>\n",
    "\n",
    "`PiecewiseConstantDecay` devuelve un callable de 1 argumento para calcular el decaimiento constante escalonado cuando se le pasa el paso actual del optimizador.\n",
    "\n",
    "```python\n",
    "from keras.optimizers.schedules import PiecewiseConstantDecay \n",
    "scheduler = PiecewiseConstantDecay(\n",
    "        boundaries, \n",
    "        values, \n",
    "        name=None\n",
    "    )\n",
    "```\n",
    "\n",
    "**Parámetros:**\n",
    "\n",
    "- `boundaries`: Una lista de Tensores o enteros o flotantes con entradas estrictamente crecientes, y con todos los elementos teniendo el mismo tipo que el paso del optimizador.\n",
    "- `values`: Una lista de Tensores o flotantes o enteros que especifica los valores para los intervalos definidos por los límites. Debería tener un elemento más que los límites, y todos los elementos deberían tener el mismo tipo.\n",
    "- `name`: Una cadena. Nombre opcional de la operación. Por defecto es 'PiecewiseConstant'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers.schedules import PiecewiseConstantDecay\n",
    "\n",
    "boundaries = [1300, 2600]\n",
    "values = [0.001, 0.0001, 0.00001]\n",
    "\n",
    "lr_schedule = PiecewiseConstantDecay(boundaries, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al especificar `boundaries = [1300, 2600]` y `values = [0.001, 0.0001, 0.00001]` estamos diciendo que la tasa de aprendizaje será 0.001 hasta el paso 1300, luego 0.0001 hasta el paso 2600 y finalmente 0.00001 después del paso 2600.\n",
    "\n",
    "Cada paso de entrenamiento sucede cuando se procesa un lote de datos.\n",
    "\n",
    "Si tenemos 1000 imágenes en el conjunto de datos y un tamaño de lote de 32, entonces cada epoca tendrá 1000/32 = 31 pasos. Por lo tanto, si queremos que el cambio de tasa de aprendizaje ocurra después de 1300 pasos, entonces necesitamos 1300/31 = 41 epocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=lr_schedule)\n",
    "\n",
    "model = LeNet5_model(num_classes=3, shape=(300, 300, 3))\n",
    "\n",
    "history = train_model(X_train, y_train, X_test, y_test, model, optimizer, epochs=25)\n",
    "\n",
    "plot_loss_accuracy(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:rgb(50,120,229)\"> 3.3 Inverse Time Decay </font>\n",
    "\n",
    "Este programador aplica la función de decaimiento inverso a un paso del optimizador, dado una tasa de aprendizaje inicial proporcionada.\n",
    "\n",
    "$$\n",
    "\\alpha = \\frac{\\alpha_0}{1+\\gamma n}\n",
    "$$\n",
    "\n",
    "donde,\n",
    "$\\alpha_0$ = tasa de aprendizaje inicial  \n",
    "$\\gamma$ = tasa de decaimiento  \n",
    "$n$ = paso / pasos_de_decaimiento\n",
    "\n",
    "```python\n",
    "from keras.optimizers.schedules import InverseTimeDecay\n",
    "\n",
    "scheduler = InverseTimeDecay(\n",
    "        initial_learning_rate, \n",
    "        decay_steps, \n",
    "        decay_rate, \n",
    "        staircase=False, \n",
    "        name=None\n",
    "    )\n",
    "```\n",
    "\n",
    "- `initial_learning_rate`: Un Tensor escalar `float32` o `float64` o un número de Python. La tasa de aprendizaje inicial.\n",
    "- `decay_steps`: Con qué frecuencia aplicar el decaimiento.\n",
    "- `decay_rate`: Un número de Python. La tasa de decaimiento.\n",
    "- `staircase`: Si aplicar el decaimiento de manera discreta, en forma de escalera, en lugar de manera continua.\n",
    "- `name`: Cadena. Nombre opcional de la operación. Por defecto es 'InverseTimeDecay'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers.schedules import InverseTimeDecay\n",
    "\n",
    "initial_learning_rate = 0.1\n",
    "decay_steps = 1\n",
    "decay_rate = 0.5\n",
    "\n",
    "lr_schedule = InverseTimeDecay(initial_learning_rate, decay_steps, decay_rate)\n",
    "\n",
    "optimizer = Adam(learning_rate=lr_schedule)\n",
    "\n",
    "model = LeNet5_model(num_classes=3, shape=(300, 300, 3))\n",
    "\n",
    "history = train_model(X_train, y_train, X_test, y_test, model, optimizer, epochs=25)\n",
    "\n",
    "plot_loss_accuracy(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:rgb(50,120,229)\"> 3.4 Exponential Decay </font>\n",
    "\n",
    "Este programador aplica una función de decaimiento exponencial a un paso del optimizador, dada una tasa de aprendizaje inicial.\n",
    "\n",
    "$$\n",
    "\\alpha = \\alpha_0*\\gamma^n\n",
    "$$\n",
    "\n",
    "donde,\n",
    "$\\alpha_0$ = tasa de aprendizaje inicial  \n",
    "$\\gamma$ = tasa de decaimiento  \n",
    "$n$ = pasos / pasos_de_decaimiento\n",
    "\n",
    "\n",
    "```python \n",
    "from keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "scheduler = ExponentialDecay(\n",
    "        initial_learning_rate, \n",
    "        decay_steps, \n",
    "        decay_rate, \n",
    "        staircase=False, \n",
    "        name=None\n",
    "    )\n",
    "```\n",
    "\n",
    "- `initial_learning_rate`: Un Tensor escalar `float32` o `float64` o un número de Python. La tasa de aprendizaje inicial.\n",
    "- `decay_steps`: Un Tensor escalar `int32` o `int64` o un número de Python. Debe ser positivo. Consulta el cálculo de decaimiento arriba.\n",
    "- `decay_rate`: Un Tensor escalar `float32` o `float64` o un número de Python. La tasa de decaimiento.\n",
    "- `staircase`: Booleano. Si es `True`, decae la tasa de aprendizaje en intervalos discretos.\n",
    "- `name`: Cadena. Nombre opcional de la operación. Por defecto es 'ExponentialDecay'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "initial_learning_rate = 0.001\n",
    "decay_rate = 0.96\n",
    "decay_steps = 100\n",
    "\n",
    "lr_schedule = ExponentialDecay(initial_learning_rate, decay_steps, decay_rate)\n",
    "\n",
    "optimizer = Adam(learning_rate=lr_schedule)\n",
    "\n",
    "model = LeNet5_model(num_classes=3, shape=(300, 300, 3))\n",
    "\n",
    "history = train_model(X_train, y_train, X_test, y_test, model, optimizer, epochs=25)\n",
    "\n",
    "plot_loss_accuracy(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(50,120,229)\"> 4. Comparación de los Programadores de Tasa de Aprendizaje </font>\n",
    "\n",
    "<center>\n",
    "    <img src=\"./images/resultados_schedulers.png\" width=1200>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(50,120,229)\"> 5. Conclusión </font>\n",
    "\n",
    "En este cuaderno, exploramos el uso de tres programadores de tasa de aprendizaje diferentes y encontramos que pueden mejorar drásticamente la velocidad de convergencia del modelo.\n",
    "\n",
    "Como suele ser el caso en el aprendizaje profundo, a menudo se requiere y se recomienda la experimentación para confirmar qué configuraciones funcionan mejor para tu problema particular. \n",
    "\n",
    "Seleccionar un optimizador y un programador de tasa de aprendizaje apropiados puede marcar una gran diferencia en la cantidad de tiempo requerido para entrenar tu modelo y a veces también puede llevar a una mayor precisión.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
