{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:rgb(50, 120, 229);\"> Transfer Learning en Keras </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este cuaderno, vamos a demostrar un ejemplo de **Transfer Learning** utilizando el conjunto de datos: **Lenguaje de Señas Americano (ASL)**. \n",
    "\n",
    "- Instanciaremos la base convolucional de la arquitectura de red VGG-16 utilizando pesos preentrenados del conjunto de datos ImageNet. \n",
    "\n",
    "- Luego, agregaremos nuestro propio clasificador denso y solo entrenaremos esa parte de la red en el conjunto de datos **ASL**. \n",
    "\n",
    "Dado que el conjunto de datos ASL contiene tipos de imágenes (señales de manos) que no forman parte del conjunto de datos ImageNet, esperamos que el aprendizaje por transferencia sea poco efectivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"./images/cnn_vgg_pretrained_base_ASL.webp\" width=\"800px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(50, 120, 229);\"> 1. Configuración inicial </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.0001\n",
    "EPOCHS = 50\n",
    "IMG_WIDTH = 224\n",
    "IMG_HEIGHT = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(50, 120, 229);\"> 2. Descargar el conjunto de datos ASL </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /root/.kaggle\n",
    "!mv kaggle.json /root/.kaggle/\n",
    "!chmod 600 /root/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d grassknoted/asl-alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q asl-alphabet.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(50, 120, 229);\"> 3. Cargar el conjunto de datos ASL </font>\n",
    "\n",
    "El conjunto de datos ASL tiene la siguiente estructura de directorios:\n",
    "\n",
    "```bash\n",
    "dataset_ASL_150/\n",
    "    |______ A/\n",
    "    |______ B/\n",
    "    |______ C/\n",
    "    |______ D/\n",
    "    |______ E/\n",
    "    |______ F/\n",
    "    |______ G/\n",
    "    |______ H/\n",
    "    |______ I/\n",
    "    |______ J/\n",
    "    |______ K/\n",
    "    |______ L/\n",
    "    |______ M/\n",
    "    |______ N/\n",
    "    |______ O/\n",
    "    |______ P/\n",
    "    |______ Q/\n",
    "    |______ R/\n",
    "    |______ S/\n",
    "    |______ T/\n",
    "    |______ U/\n",
    "    |______ V/\n",
    "    |______ W/\n",
    "    |______ X/\n",
    "    |______ Y/\n",
    "    |______ Z/\n",
    "    |______ del/\n",
    "    |______ nothing/\n",
    "    |______ space/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Este dataset no tiene un conjunto de datos de validación.**\n",
    "\n",
    "Por lo tanto, dividiremos el conjunto de datos en un conjunto de entrenamiento y un conjunto de validación especificando los parámetros de la función `image_dataset_from_directory` de TensorFlow de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import image_dataset_from_directory \n",
    "\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    \"./dataset_ASL_150/\",\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Crea un conjunto de validación con el 20% de los datos especificando el subset como \"validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "class_names = train_dataset.class_names\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for images, labels in train_dataset.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        label = labels[i]\n",
    "        class_name = label.numpy().argmax()\n",
    "        plt.title(class_name)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(50, 120, 229);\"> 4. Crea el modelo </font>\n",
    "\n",
    "En esta parte del modelo, vamos a especificar una capa de entrada y la base convolucional de la red VGG-16. (Puedes incluir Data Augmentation después de la capa de entrada si lo deseas).\n",
    "\n",
    "Por último, agregaremos un clasificador denso y compilaremos el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**model.trainable = False** significa que solo entrenaremos el clasificador denso y no la base convolucional de la red VGG-16. Esto se conoce como **Transfer Learning**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "\n",
    "base_model = VGG16(\n",
    "    include_top=False, #No incluir la capa densa\n",
    "    weights='imagenet',\n",
    "    input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)\n",
    ")\n",
    "\n",
    "#Congelar las capas del modelo base\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Flatten\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "input_layer = Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "x = preprocess_input(input_layer) # Agregar preprocesamiento de VGG16\n",
    "\n",
    "x = base_model(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "output = Dense(29, activation='softmax')(x)\n",
    "\n",
    "model = Model(input_layer, output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(50, 120, 229);\"> 5. Entrenar el modelo </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(50, 120, 229);\"> 6. Visualizar los resultados </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(50, 120, 229);\"> 7. Visualizar algunas predicciones </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "for images, labels in validation_dataset.take(1):\n",
    "    predictions = model.predict(images)\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        \n",
    "        label = labels[i]\n",
    "        class_name = label.numpy().argmax()\n",
    "        plt.title(f\"Real: {class_names[class_name]} - Predicción: {class_names[predictions[i].argmax()]}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(50, 120, 229);\"> 8. Conclusiones </font>\n",
    "\n",
    "En este cuaderno, usamos un conjunto de datos diferente para experimentar con **Transfer Learning**, y vimos que la precisión de validación en el conjunto de datos **ASL** no fue tan buena. \n",
    "\n",
    "La razón de esto es que el aprendizaje por transferencia se basó en el conjunto de datos ImageNet, que no tiene representaciones para señales de manos. Por lo tanto, la base convolucional que se utilizó para el aprendizaje por transferencia, en este caso, carece de las características de nivel inferior que serían útiles para clasificar señales de manos.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
