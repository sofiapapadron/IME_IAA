{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:rgb(50,120,229)\"> Arquitecturas De Redes Neuronales Convolucionales </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este cuaderno revisaremos la estructura de las redes neuronales convolucionales más populares utilizadas en clasicación de imágenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font style=\"color:rgb(50,120,229)\"> **Imagenet** </font>\n",
    "\n",
    "Primero recordemos el conjunto de datos ImageNet que ya discutimos en la descripción general de la tarea de clasificación. \n",
    "\n",
    "ImageNet tiene un tamaño y diversidad excepcionales, contiene más de 1.2 millones de imágenes de 1000 categorías como gatos, perros, aviones, etc. ImageNet también tiene una competencia de clasificación asociada que ha impulsado el progreso en redes neuronales durante años. \n",
    "\n",
    "ImageNet es el estándar de facto en la clasificación de imágenes y es el principal punto de referencia para comparar arquitecturas de redes neuronales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"./images/imagenet.png\" width=1000>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(50,120,229)\"> 1. AlexNet (2012) </font>\n",
    "\n",
    "Se introdujo en 2012. El artículo de AlexNet fue realmente revolucionario para la visión por computadora y la tecnología moderna en general. Este artículo fue el punto de inflexión y literalmente comenzó la revolución del aprendizaje profundo y la inteligencia artificial moderna.\n",
    "\n",
    "Su éxito fue impulsado por dos factores principales. El primero fue el enorme conjunto de datos de ImageNet y el segundo fue el hardware capaz de entrenar redes neuronales rápidamente.\n",
    "\n",
    "Alex Krizhevsky, el primer autor del artículo, optimizó el entrenamiento de la red neuronal con NVIDIA CUDA. Utilizó dos tarjetas GTX 580, que eran las GPU de NVIDIA más recientes en ese momento. Con 60 millones de parámetros, la arquitectura era tan grande que tuvo que dividirse en dos GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:rgb(50,120,229)\"> 1.1. Arquitectura de AlexNet </font>\n",
    "\n",
    "<center>\n",
    "<img src=\"./images/alexnet.png\" width=800>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(50,120,229)\"> 2. VGGNet (2014) </font>\n",
    "\n",
    "Veamos la red VGG, que se basó en gran medida en AlexNet pero con algunas diferencias importantes.\n",
    "\n",
    "<font style=\"color:rgb(50,120,229)\"> **2.1 Profundidad de la red** </font>\n",
    "\n",
    "Hay dos versiones comunes de VGG llamadas VGG16 y VGG19, con 13 y 16 capas convolucionales respectivamente. \n",
    "\n",
    "Ambas tienen tres capas completamente conectadas al final de la red, por lo que cuando las sumas, hay 16 capas y 19 capas para VGG16 y VGG19 respectivamente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font style=\"color:rgb(50,120,229)\"> **2.2 Tamaño de los filtros** </font>\n",
    "\n",
    "En lugar de los grandes núcleos convolucionales de 11 por 11 utilizados por AlexNet, VGG utiliza núcleos de tres por tres. Esto reduce significativamente el número de parámetros de la red y su tiempo de computación.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"./images/vgg.png\" width=1200>\n",
    "</center>\n",
    "\n",
    "A diferencia de AlexNet, que es notable principalmente por razones históricas, VGG fue ampliamente utilizado en muchos problemas de visión por computadora y sigue siendo utilizado en muchas tareas incluso en la actualidad debido a su estructura relativamente simple. \n",
    "\n",
    "Es fácil comprobar hipótesis, probar nuevos hiperparámetros o implementarlo en plataformas no estándar como la FPGA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(50,120,229)\"> 3. GoogLeNet (2014) </font>\n",
    "La arquitectura de GoogleNet es bastante diferente de AlexNet o VGG. En primer lugar, es más profunda.\n",
    "\n",
    "GoogleNet tiene alrededor de 22 capas convolucionales. Como puedes ver, GoogleNet consiste en bloques repetidos de esta estructura única. Este es un patrón común en las arquitecturas de redes modernas. Típicamente, los investigadores idean una nueva arquitectura de bloque y luego se repite varias veces en la red.\n",
    "\n",
    "<center>\n",
    "<img src=\"./images/googlenet.png\" width=1200>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font style=\"color:rgb(50,120,229)\"> **3.1. Inception Module** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El bloque Inception está inspirado en dos ideas. \n",
    "\n",
    "- En primer lugar, aplica filtros de uno por uno, tres por tres y cinco por cinco al mismo conjunto de entradas. De esta manera, encuentra parámetros en diferentes escalas porque estos filtros tienen diferentes campos receptivos. \n",
    "- En segundo lugar, aplica convoluciones de uno por uno con menos canales de salida a los datos para comprimir la información, y luego se aplican capas convolucionales de tres por tres y cinco por cinco sobre eso.\n",
    "\n",
    "<center>\n",
    "<img src=\"./images/inception_module.png\" width=600>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estructuras como esta se llaman cuellos de botella. Los cuellos de botella ayudan a disminuir drásticamente el número de cálculos sin sacrificar la precisión, lo que puede ayudar a entrenar los algoritmos más rápido y, por lo tanto, poder ejecutar más experimentos o acelerar la inferencia durante la implementación.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(50,120,229)\"> 4. ResNet (2015) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet fue motivado por una idea simple pero muy poderosa. Muchos investigadores de aprendizaje profundo querían hacer las redes neuronales aún más profundas, pero en la mayoría de los casos, simplemente apilar más capas no mejoraba la precisión en absoluto.\n",
    "\n",
    "El equipo de Microsoft Research Asia investigó por qué sucede esto y agregó una llamada conexión de salto o conexión residual.\n",
    "\n",
    "<center>\n",
    "<img src=\"./images/resnet.png\" width=800>\n",
    "</center>\n",
    "\n",
    "Vamos a entrar en un ejemplo sencillo para explicarlo. \n",
    "\n",
    "Imagina que hay una imagen simple de un perro en el conjunto de entrenamiento, y algo así se puede manejar fácilmente usando una red bastante superficial. \n",
    "\n",
    "Entonces, si alguna característica de una capa temprana es capaz de clasificar correctamente este perro, lo que queremos es que el resto de la red actúe como una transformación de identidad.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(50,120,229)\"> ¿Cómo construir mi propia arquitectura de red neuronal convolucional? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando crees tu propia solución, te recomendamos que comiences con una arquitectura conocida, por ejemplo, puedes elegir un algoritmo de última generación, preferiblemente con una buena implementación de código abierto y pesos pre-entrenados disponibles. La razón por la que los pesos pre-entrenados son tan importantes es porque podemos usar estos pesos como punto de partida.\n",
    "\n",
    "Así que lo primero que intentamos es el ajuste fino, donde realizamos cambios mínimos en la arquitectura de la red y tratamos de usar los pesos pre-entrenados para resolver el problema.\n",
    "\n",
    "Pero si eso no produce buenos resultados, es posible que desees ajustar el algoritmo o la arquitectura. Por ejemplo, podrías agregar más bloques de red, cambiar los parámetros de la capa convolucional o incluso agregar otros tipos de capas como otras capas de activación."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
